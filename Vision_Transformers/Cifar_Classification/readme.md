I have implemented Vision Transformers (ViT) from scratch for Cifar Classification dataset using Pytorch

The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.
There are 50000 training images and 10000 test images.

I have trained it on only 10 epochs just to see the basic working of the transformer.

It contains code of all the major components inlcuding the Encoder block, patch + position embeddings and the MLP head.
This is not for production use, just to understand the inner workings of a Transformer for research purposes and to improve upon this architecture
