{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToSrpdaavKKV",
        "outputId": "26970dc7-29e2-4d7c-c924-b48420d5982b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qe0dvR4G86To"
      },
      "outputs": [],
      "source": [
        "#importing reqd libraries\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VoldPgOOLxuW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "# import torchvision\n",
        "\n",
        "# Mount your Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to your data directory\n",
        "# data_dir = \"/home/patel.avni/Plastics_Classification\"\n",
        "\n",
        "# Define the batch size and image size\n",
        "# batch_size = 16\n",
        "# img_size = (224, 224)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "mNOFe2uwvKKZ",
        "outputId": "defbee59-3773-4ba2-94a7-4395329bd314"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ce31dcbe888f>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mtrain_class_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/Computer_Vision/Final_project_Computer_vision/Plastics_Classification_3/train'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import random\n",
        "import shutil\n",
        "from torch.utils.data import  SubsetRandomSampler\n",
        "# Set the path to your data directory\n",
        "data_dir = \"E:/Computer_Vision/Final_project_Computer_vision/Plastics_Classification_3\"\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.7552, 0.7162, 0.6846),(0.2468, 0.2586, 0.2788))\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.7532, 0.7138, 0.6792),(0.2489, 0.2587, 0.2797))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.7591, 0.7196, 0.6895),(0.2456, 0.2572, 0.2756))\n",
        "])\n",
        "\n",
        "\n",
        "# Compute the mean and standard deviation of the train dataset\n",
        "# Load the train and test sets\n",
        " # create two dataset objects, train_dataset and val_dataset, from the image data located in the train directory within the data_dir.\n",
        "\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "val_dir = os.path.join(data_dir, \"val\")\n",
        "test_dir = os.path.join(data_dir, \"test\")\n",
        "\n",
        "classes = [\"heavy_plastic\", \"no_image\", \"no_plastic\", \"some_plastic\"]\n",
        "\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transforms)\n",
        "\n",
        "train_class_counts = [0] * len(train_dataset.classes)\n",
        "for _, label in train_dataset:\n",
        "    train_class_counts[label] += 1\n",
        "\n",
        "train_class_weights = [1.0 / count for count in train_class_counts]\n",
        "train_sample_weights = [train_class_weights[label] for _, label in train_dataset]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(np.arange(len(train_dataset)))\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, sampler=train_sampler)\n",
        "\n",
        "# Create the validation dataset and data loader\n",
        "val_dataset = ImageFolder(val_dir, transform=val_transforms)\n",
        "\n",
        "val_class_counts = [0] * len(val_dataset.classes)\n",
        "for _, label in val_dataset:\n",
        "    val_class_counts[label] += 1\n",
        "\n",
        "val_class_weights = [1.0 / count for count in val_class_counts]\n",
        "val_sample_weights = [val_class_weights[label] for _, label in val_dataset]\n",
        "\n",
        "val_sampler = SubsetRandomSampler(np.arange(len(val_dataset)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, sampler=val_sampler)\n",
        "\n",
        "\n",
        "# Create the test dataset and data loader\n",
        "test_dataset = ImageFolder(test_dir, transform=test_transforms)\n",
        "\n",
        "test_class_counts = [0] * len(test_dataset.classes)\n",
        "for _, label in test_dataset:\n",
        "    test_class_counts[label] += 1\n",
        "\n",
        "test_class_weights = [1.0 / count for count in test_class_counts]\n",
        "test_sample_weights = [test_class_weights[label] for _, label in test_dataset]\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKA6Gk7OvKKZ"
      },
      "outputs": [],
      "source": [
        "# Compute the class weights for loss function\n",
        "class_count = [0] * len(classes)\n",
        "for _, labels in train_dataset:\n",
        "    for i in range(len(classes)):\n",
        "        if labels == i:\n",
        "            class_count[i] += 1\n",
        "total_count = sum(class_count)\n",
        "class_weights = [total_count / (len(classes) * count) for count in class_count]\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x_FlyjbvKKa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
        "    best_val_acc=0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_total = 0\n",
        "        train_correct = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += targets.size(0)\n",
        "                val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "        # Update the learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "        # Print the training and validation loss and accuracy\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = 100.0 * train_correct / train_total\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            torch.save(model.state_dict(),'best_model.pth')\n",
        "            best_val_acc=val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_JUbE3DCQG2"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The PyTorch model to evaluate.\n",
        "        dataloader (torch.utils.data.DataLoader): The PyTorch DataLoader object to use for iterating over the dataset.\n",
        "\n",
        "    Returns:\n",
        "        float: The accuracy of the model on the given dataset.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8AAyrpMvKKa"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights = torch.tensor(class_weights, device=device) # move to CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCFq9OJgofRd"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import random\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import optuna\n",
        "import torch.nn.init as init\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "\n",
        "# Define the hyperparameters to search over\n",
        "#     learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
        "#     weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)\n",
        "#     dropout = trial.suggest_uniform(\"dropout\", 0.0, 0.5)\n",
        "    # epochs = trial.suggest_int(\"epochs\", 5, 20)\n",
        "    # step_size = trial.suggest_int(\"step_size\", 1, 10)\n",
        "    # gamma = trial.suggest_loguniform(\"gamma\", 1e-5, 1e-2)\n",
        "#     batch_size = trial.suggest_categorical(\"batch_size\", [ 4,8])\n",
        "\n",
        "    # Create data loaders with the specified batch size\n",
        "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Create a new instance of the EfficientNet-B0 model\n",
        "efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "# Modify the model\n",
        "in_features = efficientnet._fc.in_features\n",
        "efficientnet._fc = nn.Sequential(\n",
        "    nn.Dropout(0.1693343055004149),\n",
        "    nn.Linear(in_features, 4)\n",
        ")\n",
        "\n",
        "# Move the model and data to the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "efficientnet = efficientnet.to(device)\n",
        "\n",
        "# Define the loss function, optimizer and learning rate scheduler\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights))\n",
        "\n",
        "#     if optimizer_name == \"Adam\":\n",
        "#         optimizer = optim.Adam(efficientnet.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "optimizer = optim.Adamax(efficientnet.parameters(), lr= 0.00045757341361, weight_decay=0.0016725928825570783)\n",
        "#     elif optimizer_name == \"Adagrad\":\n",
        "#         optimizer = optim.Adagrad(efficientnet.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=8, gamma=0.1)\n",
        "\n",
        "# Train the model\n",
        "train_model(efficientnet, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_acc = evaluate_model(efficientnet, val_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # study = optuna.create_study(direction=\"maximize\")\n",
        "# study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# # Print the best validation accuracy and best hyperparameters\n",
        "# best_trial = study.best_trial\n",
        "# print(f'Best validation accuracy: {best_trial.value:.4f}')\n",
        "# print(\"Best hyperparameters:\")\n",
        "# for key, value in best_trial.params.items():\n",
        "#     print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGOZIH0f86Tw"
      },
      "outputs": [],
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_dir = \"E:/Computer_Vision/Final_project_Computer_vision/Plastics_Classification_3\"\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.7591, 0.7196, 0.6895),(0.2456, 0.2572, 0.2756))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to compute the accuracy of the model on a given dataset\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Disable gradient tracking during inference\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Load the test dataset\n",
        "# test_dataset = MyDataset(test_data, test_labels)\n",
        "test_dataset= ImageFolder(os.path.join(data_dir, \"test\"), transform=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Load the best model found by Optuna\n",
        "efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "in_features = efficientnet._fc.in_features\n",
        "efficientnet._fc = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(in_features, 4)\n",
        ")\n",
        "efficientnet.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "efficientnet = efficientnet.to(device)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_acc = evaluate_model(efficientnet, test_loader)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfG7b-81vKKn"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of batches in test loader: {len(test_loader)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPsWexMLvKKn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a dictionary that maps label indices to class names\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "efficientnet.eval()\n",
        "\n",
        "test_iterator = iter(test_loader)\n",
        "test_data = [next(test_iterator) for i in range(150)]\n",
        "\n",
        "for i, (images, labels) in enumerate(test_data):\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = efficientnet(images)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    images = images.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "    probs = probs.cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(10, 5))\n",
        "    fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "    for j, image in enumerate(images):\n",
        "        pred_class = class_names[preds[j]]\n",
        "        true_class = class_names[labels[j]]\n",
        "        confidence = probs[j, preds[j]]\n",
        "        axs[j // 4, j % 4].imshow(np.transpose(image, (1, 2, 0)))\n",
        "        axs[j // 4, j % 4].set_title(f\"Predicted: {pred_class}\\nTrue: {true_class}\\nConfidence: {confidence:.3f}\")\n",
        "        axs[j // 4, j % 4].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC89HqVPvKKn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from captum.attr import GuidedBackprop\n",
        "\n",
        "# Define a GuidedBackprop attribution algorithm\n",
        "guided_backprop = GuidedBackprop(efficientnet)\n",
        "\n",
        "# Choose an image index for which to generate a saliency map\n",
        "image_index = 0\n",
        "\n",
        "# Get the corresponding image and label from the test data\n",
        "image, label = test_data[image_index]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image = image.to(device)\n",
        "label = label.to(device)\n",
        "\n",
        "# Extract the first image from the batch and add a batch dimension\n",
        "image = image[0].unsqueeze(0)\n",
        "\n",
        "# Get the predicted class probabilities for the given image\n",
        "with torch.no_grad():\n",
        "    output = efficientnet(image)\n",
        "    probs = nn.functional.softmax(output, dim=1)\n",
        "\n",
        "# Get the index of the class with the highest probability\n",
        "target_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "# Calculate the saliency map for the chosen image\n",
        "saliency_map = guided_backprop.attribute(image, target=target_class)\n",
        "saliency_map = saliency_map.to(device)\n",
        "\n",
        "# Convert the saliency map to a grayscale image and show it\n",
        "# print(\"Input image shape:\", image.shape)  # Debugging line\n",
        "saliency_map = np.transpose(saliency_map.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
        "saliency_map = np.maximum(saliency_map, 0)\n",
        "saliency_map /= saliency_map.max()\n",
        "plt.imshow(saliency_map, cmap=\"gray\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBmZfez9vKKo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from captum.attr import GuidedBackprop\n",
        "\n",
        "# Define a GuidedBackprop attribution algorithm\n",
        "guided_backprop = GuidedBackprop(efficientnet)\n",
        "\n",
        "# Generate saliency maps for all images in the test loader\n",
        "for image, label in test_loader:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    image = image.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    # Extract the first image from the batch and add a batch dimension\n",
        "    image = image[0].unsqueeze(0)\n",
        "\n",
        "    # Get the predicted class probabilities for the given image\n",
        "    with torch.no_grad():\n",
        "        output = efficientnet(image)\n",
        "        probs = nn.functional.softmax(output, dim=1)\n",
        "\n",
        "    # Get the index of the class with the highest probability\n",
        "    target_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "    # Calculate the saliency map for the chosen image\n",
        "    saliency_map = guided_backprop.attribute(image, target=target_class)\n",
        "    saliency_map = saliency_map.to(device)\n",
        "\n",
        "    # Convert the saliency map to a grayscale image and show it\n",
        "    saliency_map = np.transpose(saliency_map.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
        "    saliency_map = np.maximum(saliency_map, 0)\n",
        "    saliency_map /= saliency_map.max()\n",
        "    plt.imshow(saliency_map, cmap=\"gray\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyNXfD4AvKKo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from captum.attr import GuidedBackprop\n",
        "\n",
        "# Define a GuidedBackprop attribution algorithm\n",
        "guided_backprop = GuidedBackprop(efficientnet)\n",
        "\n",
        "# Generate saliency maps for all images in the test loader\n",
        "for image, label in test_loader:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    image = image.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    # Extract the first image from the batch and add a batch dimension\n",
        "    image = image[0].unsqueeze(0)\n",
        "\n",
        "    # Get the predicted class probabilities for the given image\n",
        "    with torch.no_grad():\n",
        "        output = efficientnet(image)\n",
        "        probs = nn.functional.softmax(output, dim=1)\n",
        "\n",
        "    # Get the index of the class with the highest probability\n",
        "    target_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "    # Calculate the saliency map for the chosen image\n",
        "    saliency_map = guided_backprop.attribute(image, target=target_class)\n",
        "    saliency_map = saliency_map.to(device)\n",
        "\n",
        "    # Convert the saliency map and original image to numpy arrays\n",
        "    saliency_map = np.transpose(saliency_map.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
        "    saliency_map = np.maximum(saliency_map, 0)\n",
        "    saliency_map /= saliency_map.max()\n",
        "    original_image = np.transpose(image.squeeze().cpu().detach().numpy(), (1, 2, 0))\n",
        "\n",
        "    # Create a figure with two subplots for the original image and saliency map\n",
        "    fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
        "\n",
        "    # Display the original image in the left subplot\n",
        "    axs[0].imshow(original_image)\n",
        "    axs[0].set_title('Original Image')\n",
        "\n",
        "    # Display the saliency map in the right subplot\n",
        "    axs[1].imshow(saliency_map, cmap=\"gray\")\n",
        "    axs[1].set_title('Saliency Map')\n",
        "\n",
        "    # Show the figure\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gBOidKdvKKo"
      },
      "outputs": [],
      "source": [
        "print(\"Input image shape:\", image.shape)\n",
        "print(\"Input image shape:\", label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U0fCdjzvKKo"
      },
      "outputs": [],
      "source": [
        "def get_embedding(model, dataloader):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create empty lists to store embeddings and labels\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate over the data in the dataloader\n",
        "    with torch.no_grad():\n",
        "        for images, target in dataloader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Get the embeddings from the model\n",
        "            features = model.extract_features(images)\n",
        "            embeddings.append(features.cpu().numpy())\n",
        "\n",
        "            # Get the labels\n",
        "            labels.append(target.cpu().numpy())\n",
        "\n",
        "    # Concatenate the embeddings and labels into a single array\n",
        "    embeddings = np.concatenate(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "\n",
        "    return embeddings, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3iPnqf6vKKo"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "efficientnet.eval()\n",
        "\n",
        "# Get the embeddings of the test set\n",
        "embeddings = []\n",
        "labels = []\n",
        "images = []\n",
        "with torch.no_grad():\n",
        "    for batch_images, batch_labels in test_loader:\n",
        "        batch_images = batch_images.to(device)\n",
        "        batch_embeddings = efficientnet(batch_images)\n",
        "        embeddings.append(batch_embeddings.cpu().numpy())\n",
        "        labels.append(batch_labels.cpu().numpy())\n",
        "        images.append(batch_images.cpu().numpy())\n",
        "embeddings = np.concatenate(embeddings, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "images = np.concatenate(images, axis=0)\n",
        "\n",
        "# Normalize the images and convert to uint8\n",
        "images = (images * 255).astype(np.uint8)\n",
        "\n",
        "# Perform t-SNE dimensionality reduction\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "embeddings_tsne = tsne.fit_transform(embeddings)\n",
        "\n",
        "# Visualize the embeddings\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "for i in range(len(class_names)):\n",
        "    indices = np.where(labels == i)[0]\n",
        "    x = embeddings_tsne[indices, 0]\n",
        "    y = embeddings_tsne[indices, 1]\n",
        "    ax.scatter(x, y, label=class_names[i], alpha=0.5)\n",
        "plt.title(\"t-SNE Visualization of Test Set Embeddings\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bV-5cNfvKKo"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import visdom\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Create a Visdom server instance\n",
        "viz = visdom.Visdom()\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "efficientnet.eval()\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "# Get the embeddings of the test set\n",
        "embeddings = []\n",
        "labels = []\n",
        "images = []\n",
        "with torch.no_grad():\n",
        "    for batch_images, batch_labels in test_loader:\n",
        "        batch_images = batch_images.to(device)\n",
        "        batch_embeddings = efficientnet(batch_images)\n",
        "        embeddings.append(batch_embeddings.cpu().numpy())\n",
        "        labels.append(batch_labels.cpu().numpy())\n",
        "        images.append(batch_images.cpu().numpy())\n",
        "embeddings = np.concatenate(embeddings, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "images = np.concatenate(images, axis=0)\n",
        "\n",
        "# Normalize the images and convert to uint8\n",
        "images = (images * 255).astype(np.uint8)\n",
        "\n",
        "# Perform t-SNE dimensionality reduction\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "embeddings_tsne = tsne.fit_transform(embeddings)\n",
        "embeddings_tsne = embeddings_tsne.reshape(-1, 2)\n",
        "\n",
        "\n",
        "# Add 1 to the labels to ensure they start from 1\n",
        "labels += 1\n",
        "\n",
        "# Visualize the embeddings in Visdom\n",
        "scatter_data = []\n",
        "for i in range(len(class_names)):\n",
        "    indices = np.where(labels == i)[0]\n",
        "    x = embeddings_tsne[indices, 0]\n",
        "    y = embeddings_tsne[indices, 1]\n",
        "    label = class_names[i]\n",
        "    scatter_data.append({'x': x, 'y': y, 'name': label, 'mode': 'markers', 'marker': {'size': 5, 'opacity': 0.5}})\n",
        "viz.scatter(X=embeddings_tsne, Y=labels, opts=dict(title='t-SNE Visualization of Test Set Embeddings', markersize=5, xlabel='x', ylabel='y',legend=list(class_names.values())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgzq9nhCvKKo"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "import visdom\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Create a Visdom server instance\n",
        "viz = visdom.Visdom()\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "efficientnet.eval()\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "# Get the embeddings of the test set\n",
        "embeddings = []\n",
        "labels = []\n",
        "images = []\n",
        "with torch.no_grad():\n",
        "    for batch_images, batch_labels in test_loader:\n",
        "        batch_images = batch_images.to(device)\n",
        "        batch_embeddings = efficientnet(batch_images)\n",
        "        embeddings.append(batch_embeddings.cpu().numpy())\n",
        "        labels.append(batch_labels.cpu().numpy())\n",
        "        images.append(batch_images.cpu().numpy())\n",
        "embeddings = np.concatenate(embeddings, axis=0)\n",
        "labels = np.concatenate(labels, axis=0)\n",
        "images = np.concatenate(images, axis=0)\n",
        "\n",
        "# Normalize the images and convert to uint8\n",
        "images = (images * 255).astype(np.uint8)\n",
        "\n",
        "# Perform UMAP dimensionality reduction\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "embeddings_umap = umap_reducer.fit_transform(embeddings)\n",
        "\n",
        "# Add 1 to the labels to ensure they start from 1\n",
        "labels += 1\n",
        "\n",
        "# Visualize the embeddings in Visdom\n",
        "scatter_data = []\n",
        "for i in range(len(class_names)):\n",
        "    indices = np.where(labels == i)[0]\n",
        "    x = embeddings_umap[indices, 0]\n",
        "    y = embeddings_umap[indices, 1]\n",
        "    label = class_names[i]\n",
        "    scatter_data.append({'x': x, 'y': y, 'name': label, 'mode': 'markers', 'marker': {'size': 5, 'opacity': 0.5}})\n",
        "viz.scatter(X=embeddings_umap, Y=labels, opts=dict(title='UMAP Visualization of Test Set Embeddings', markersize=5, xlabel='x', ylabel='y',legend=list(class_names.values())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTGblKO2vKKo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a dictionary that maps label indices to class names\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "# Set the model to evaluation model\n",
        "efficientnet.to(device)\n",
        "efficientnet.eval()\n",
        "\n",
        "# Define the different sizes of datasets to evaluate, from 10 to the maximum size of the test dataset\n",
        "dataset_sizes = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1342]\n",
        "\n",
        "# Create a dictionary to store the evaluation results\n",
        "evaluation_results = {}\n",
        "\n",
        "for size in dataset_sizes:\n",
        "    # Select a random subset of the test dataset\n",
        "    test_subset = torch.utils.data.random_split(test_dataset, [size, len(test_dataset) - size])[0]\n",
        "    test_loader = torch.utils.data.DataLoader(test_subset, batch_size=8, shuffle=False)\n",
        "\n",
        "    test_iterator = iter(test_loader)\n",
        "    test_data = [next(test_iterator) for i in range(len(test_loader))]\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_data):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = efficientnet(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_predictions += len(labels)\n",
        "\n",
        "    accuracy = correct_predictions.double() / total_predictions\n",
        "    evaluation_results[size] = accuracy.item()\n",
        "\n",
        "# Plot the evaluation results\n",
        "plt.plot(list(evaluation_results.keys()), list(evaluation_results.values()))\n",
        "plt.xlabel('Dataset size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLbb6_dkvKKp"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Define epsilon for FGSM\n",
        "epsilon = 0.8\n",
        "\n",
        "# Define a dictionary that maps label indices to class names\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "test_iterator = iter(test_loader)\n",
        "test_data = [next(test_iterator) for i in range(7)]\n",
        "\n",
        "for i, (images, labels) in enumerate(test_data):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Evaluate the model's accuracy on original examples\n",
        "    with torch.no_grad():\n",
        "        outputs = efficientnet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "    accuracy_original = torch.sum(preds == labels).item() / len(labels) *100\n",
        "\n",
        "    # Generate adversarial examples using FGSM\n",
        "    images.requires_grad = True\n",
        "    outputs = efficientnet(images)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    loss.backward()\n",
        "    images_grad = torch.sign(images.grad)\n",
        "    images_adversarial = images + epsilon * images_grad\n",
        "    images_adversarial = torch.clamp(images_adversarial, 0, 1)\n",
        "\n",
        "    # Evaluate the model's accuracy on adversarial examples\n",
        "    with torch.no_grad():\n",
        "        outputs = efficientnet(images_adversarial)\n",
        "        _, preds_adv = torch.max(outputs, 1)\n",
        "    accuracy_adversarial = torch.sum(preds_adv == labels).item() / len(labels) *100\n",
        "\n",
        "    images = images.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    preds_adv = preds_adv.detach().cpu().numpy()\n",
        "    images_adversarial = images_adversarial.detach().cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=2, ncols=len(images), figsize=(15, 5))\n",
        "    fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "    for j, image in enumerate(images):\n",
        "        pred_class = class_names[preds[j]]\n",
        "        true_class = class_names[labels[j]]\n",
        "        axs[0, j].imshow(np.transpose(image, (1, 2, 0)))\n",
        "        axs[0, j].set_title(f\"Original\\nPredicted: {pred_class}\\nTrue: {true_class}\",fontsize=8)\n",
        "        axs[0, j].axis(\"off\")\n",
        "\n",
        "    for j, image in enumerate(images_adversarial):\n",
        "        pred_class_adv = class_names[preds_adv[j]]\n",
        "        true_class_adv = class_names[labels[j]]\n",
        "        axs[1, j].imshow(np.transpose(image, (1, 2, 0)))\n",
        "        axs[1, j].set_title(f\"Adversarial\\nPredicted: {pred_class_adv}\\nTrue: {true_class_adv}\",fontsize=8)\n",
        "        axs[1, j].axis(\"off\")\n",
        "\n",
        "    print(f\"Accuracy on original examples: {accuracy_original:.4f}%\")\n",
        "    print(f\"Accuracy on adversarial examples: {accuracy_adversarial:.4f}%\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws9P_Kh4vKKp"
      },
      "outputs": [],
      "source": [
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = efficientnet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels += labels.cpu().numpy().tolist()\n",
        "        predicted_labels += preds.cpu().numpy().tolist()\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\", xticklabels=class_names.values(), yticklabels=class_names.values())\n",
        "plt.title(\"Confusion Matrix for Test Set\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyzunGOYvKKp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "AY5GLL6gvKKp"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "\n",
        "# # Define a dictionary that maps label indices to class names\n",
        "# class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "# # Set the model to evaluation mode\n",
        "# efficientnet.eval()\n",
        "\n",
        "# # Define the loss function as the cross-entropy loss\n",
        "# loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# # Define a function to compute the saliency map\n",
        "# def compute_saliency_map(inputs, target_class):\n",
        "#     inputs.requires_grad_()\n",
        "#     outputs = efficientnet(inputs)\n",
        "#     loss = loss_fn(outputs, torch.tensor(target_class).to(device))\n",
        "#     loss.backward()\n",
        "#     saliency_map = torch.abs(inputs.grad).max(dim=1)[0]\n",
        "#     saliency_map = torch.stack([saliency_map]*3, dim=1)\n",
        "#     return saliency_map\n",
        "\n",
        "# test_iterator = iter(test_loader)\n",
        "# test_data = [next(test_iterator) for i in range(7)]\n",
        "\n",
        "# for i, (images, labels) in enumerate(test_data):\n",
        "#     images = images.to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = efficientnet(images)\n",
        "#         _, preds = torch.max(outputs, 1)\n",
        "\n",
        "#     images = images.cpu().numpy()\n",
        "#     labels = labels.cpu().numpy()\n",
        "#     preds = preds.cpu().numpy()\n",
        "\n",
        "#     fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(10, 7))\n",
        "#     fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "#     for j, image in enumerate(images):\n",
        "#         if j >= axs.shape[1]:\n",
        "#             break\n",
        "#         pred_class = class_names[preds[j]]\n",
        "#         true_class = class_names[labels[j]]\n",
        "#         axs[0, j].imshow(np.transpose(image, (1, 2, 0)))\n",
        "#         axs[0, j].set_title(f\"Predicted: {pred_class}\\nTrue: {true_class}\")\n",
        "#         axs[0, j].axis(\"off\")\n",
        "\n",
        "#         # Compute the saliency map\n",
        "#         saliency_map = compute_saliency_map(torch.from_numpy(images).clone().to(device).requires_grad_(), torch.from_numpy(labels).clone().to(device))\n",
        "\n",
        "#         axs[1, j].imshow(np.transpose(saliency_map[j].cpu().numpy(), (1, 2, 0)))\n",
        "\n",
        "#         axs[1, j].set_title(\"Saliency Map\")\n",
        "#         axs[1, j].axis(\"off\")\n",
        "\n",
        "#         heatmap = np.uint8(255 * np.transpose(saliency_map.cpu().numpy()[j], (1, 2, 0)))\n",
        "#         heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "# #         plt.imshow(heatmap, cmap='jet')\n",
        "# #         plt.axis('off')\n",
        "#         heatmap = np.float32(heatmap) / 255\n",
        "#         heatmap = heatmap[..., ::-1]  # Convert BGR to RGB\n",
        "#         superimposed_img = np.float32(np.transpose(image, (1, 2, 0))) / 255 + 0.5 * heatmap\n",
        "#         superimposed_img /= superimposed_img.max()\n",
        "#         axs[2, j].imshow(np.transpose(superimposed_img, (0, 1, 2)))\n",
        "#         axs[2, j].set_title(\"Superimposed Image\")\n",
        "#         axs[2, j].axis(\"off\")\n",
        "\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRI5RRyVvKKp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBdmMfffvKKp"
      },
      "outputs": [],
      "source": [
        "print(images.shape) # prints the shape of the image array\n",
        "print(saliency_map.shape) # prints the shape of the saliency map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWHKEsVivKKp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the EfficientNet model\n",
        "efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "efficientnet.to(device)\n",
        "\n",
        "# Visualize the filters of the first convolutional layer\n",
        "conv1_filters = efficientnet._conv_stem.weight.data.cpu()\n",
        "grid = vutils.make_grid(conv1_filters, nrow=8, normalize=True, scale_each=True)\n",
        "\n",
        "# Show the grid of filters\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Filters of the first convolutional layer\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDn6wwBWvKKp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the EfficientNet model\n",
        "# efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "# efficientnet.to(device)\n",
        "\n",
        "# Define the convolutional layers to visualize\n",
        "conv_layers = [efficientnet._conv_stem, efficientnet._blocks[0]._depthwise_conv,\n",
        "               efficientnet._blocks[1]._depthwise_conv]\n",
        "\n",
        "# Visualize the filters of each convolutional layer\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "for i, layer in enumerate(conv_layers):\n",
        "    filters = layer.weight.data.cpu()\n",
        "    grid = vutils.make_grid(filters, nrow=8, normalize=True, scale_each=True)\n",
        "    ax = fig.add_subplot(1, len(conv_layers), i+1)\n",
        "    ax.imshow(np.transpose(grid, (1, 2, 0)))\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Filters of Conv{i+1}\")\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rfI75_VvKKp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "efficientnet.eval()\n",
        "\n",
        "# Initialize lists to store the true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate over the test data\n",
        "for images, labels in test_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = efficientnet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "    predicted_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Convert the true labels and predicted labels to numpy arrays\n",
        "true_labels = np.array(true_labels)\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "# Calculate the classification report, which includes precision, recall, F1-score, and support\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names.values())\n",
        "\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6akJC5IvKKp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "print(cm)\n",
        "# Create a heatmap from the confusion matrix\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lyGa3ECvKKp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shap\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Define a dictionary that maps label indices to class names\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "efficientnet.eval()\n",
        "\n",
        "# Load the test data into a list\n",
        "test_images = []\n",
        "test_iterator = iter(test_loader)\n",
        "for i in range(len(test_loader)):\n",
        "    images, _ = next(test_iterator)\n",
        "    test_images.append(images)\n",
        "\n",
        "# Convert the list to a numpy array\n",
        "test_images = np.concatenate(test_images, axis=0)\n",
        "\n",
        "# Remove the first dimension\n",
        "test_images = test_images[0]\n",
        "\n",
        "# Convert the numpy array to a PyTorch tensor\n",
        "to_tensor = transforms.ToTensor()\n",
        "test_images = to_tensor(test_images)\n",
        "# test_images = test_images.numpy()\n",
        "\n",
        "# Load the SHAP explainer\n",
        "explainer = shap.KernelExplainer(efficientnet, test_images.numpy())\n",
        "\n",
        "test_images = test_images.numpy()\n",
        "test_iterator = iter(test_loader)\n",
        "test_data = [next(test_iterator) for i in range(7)]\n",
        "\n",
        "for i, (images, labels) in enumerate(test_data):\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = efficientnet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    images = images.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    # Compute the SHAP values for the first 5 images in the batch\n",
        "    shap_values = explainer.shap_values(images[:5])\n",
        "\n",
        "    # Plot the original images and their corresponding SHAP values side by side\n",
        "    fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(8, 16))\n",
        "    for j in range(5):\n",
        "        axs[j, 0].imshow(np.transpose(images[j], (1, 2, 0)))\n",
        "        axs[j, 0].axis('off')\n",
        "        axs[j, 0].set_title(f\"Original image for predicted class: {class_names[preds[j]]}\")\n",
        "\n",
        "        shap.image_plot(shap_values[j], np.transpose(images[j], (1, 2, 0)), show=False, ax=axs[j, 1])\n",
        "        axs[j, 1].axis('off')\n",
        "        axs[j, 1].set_title(f\"SHAP values for predicted class: {class_names[preds[j]]}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgVGXUv4vKKq"
      },
      "outputs": [],
      "source": [
        "print(test_images.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cKb2dXbvKKq"
      },
      "outputs": [],
      "source": [
        "# from cam import cam\n",
        "\n",
        "# # Define a dictionary that maps label indices to class names\n",
        "# class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "# test_iterator = iter(test_loader)\n",
        "# test_data = [next(test_iterator) for i in range(7)]\n",
        "\n",
        "# for i, (images, labels) in enumerate(test_data):\n",
        "#     images = images.to(device)\n",
        "\n",
        "#     fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(10, 5))\n",
        "#     fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "#     for j, image in enumerate(images):\n",
        "#         # Generate the CAM heatmap\n",
        "#         heatmap = cam(image.unsqueeze(0), efficientnet)\n",
        "\n",
        "#         # Apply the heatmap to the original image\n",
        "#         image = image.cpu().numpy().transpose(1, 2, 0)\n",
        "#         image = cv2.resize(image, (224, 224))\n",
        "#         heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "#         heatmap = np.float32(heatmap) / 255\n",
        "#         image_with_heatmap = heatmap + np.float32(image)\n",
        "#         image_with_heatmap = image_with_heatmap / np.max(image_with_heatmap)\n",
        "\n",
        "#         # Get the predicted and true classes\n",
        "#         with torch.no_grad():\n",
        "#             outputs = efficientnet(torch.unsqueeze(image, 0))\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#         pred_class = class_names[preds[0].item()]\n",
        "#         true_class = class_names[labels[j]]\n",
        "\n",
        "#         # Plot the image with heatmap and label\n",
        "#         axs[j // 4, j % 4].imshow(image_with_heatmap)\n",
        "#         axs[j // 4, j % 4].set_title(f\"Predicted: {pred_class}\\nTrue: {true_class}\")\n",
        "#         axs[j // 4, j % 4].axis(\"off\")\n",
        "\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzFFKP2RvKKq"
      },
      "outputs": [],
      "source": [
        "def gradcam(image, model, layer_name):\n",
        "    \"\"\"\n",
        "    Generates a GradCAM saliency map for the input image.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): input image tensor.\n",
        "        model (torch.nn.Module): trained PyTorch model.\n",
        "        layer_name (str): name of the layer to generate the heatmap from.\n",
        "\n",
        "    Returns:\n",
        "        heatmap (np.ndarray): GradCAM heatmap.\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Get the image size\n",
        "    _, _, h, w = image.shape\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image)\n",
        "\n",
        "    # Get the index of the predicted class\n",
        "    _, pred_idx = torch.max(output, 1)\n",
        "\n",
        "    # Get the weights of the output layer\n",
        "    weights = list(model.parameters())[-2]\n",
        "\n",
        "    # Get the feature maps from the specified layer\n",
        "    activations = {}\n",
        "    def forward_hook(module, input, output):\n",
        "        activations[layer_name] = output.detach()\n",
        "    handle = model._modules[layer_name].register_forward_hook(forward_hook)\n",
        "    feature_maps = model(image)\n",
        "    handle.remove()\n",
        "\n",
        "    # Calculate the gradients\n",
        "    one_hot = torch.zeros_like(output)\n",
        "    one_hot[0][pred_idx] = 1\n",
        "    output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "    # Calculate the importance weights\n",
        "    importance_weights = torch.mean(model._modules[layer_name].weight.grad, axis=[2,3])\n",
        "\n",
        "    # Calculate the heatmap\n",
        "    heatmap = torch.zeros((h, w)).to(device)\n",
        "    for i, weight in enumerate(importance_weights[0]):\n",
        "        heatmap += weight * activations[layer_name][0, i, :, :]\n",
        "\n",
        "    # Normalize the heatmap\n",
        "    heatmap = np.maximum(heatmap.detach().cpu(), 0)\n",
        "    heatmap /= torch.max(heatmap)\n",
        "\n",
        "    return heatmap.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KfHN6dfvKKq"
      },
      "outputs": [],
      "source": [
        "# def find_target_layer(model):\n",
        "#     \"\"\"Find the final convolutional layer in the model\"\"\"\n",
        "#     for name, module in model.named_modules():\n",
        "#         if isinstance(module, nn.Conv2d):\n",
        "#             if \"classifier\" in name:\n",
        "#                 return module\n",
        "#     raise ValueError(\"Could not find the target layer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JGRzQWGvKKq"
      },
      "outputs": [],
      "source": [
        "# def find_target_layer(model):\n",
        "#     # attempt to find the final convolutional layer in the network\n",
        "#     # by looping over the layers of the network in reverse order\n",
        "#     for layer in reversed(model.layers):\n",
        "#         # check to see if the layer has a 4D output\n",
        "#         if len(layer.output_shape) == 4:\n",
        "#             return layer.name\n",
        "#     # otherwise, we could not find a 4D layer so the GradCAM\n",
        "#     # algorithm cannot be applied\n",
        "#     raise ValueError(\"Could not find 4D layer. Cannot apply GradCAM.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChB5q8z0vKKq"
      },
      "outputs": [],
      "source": [
        "def find_target_layer(model):\n",
        "    \"\"\"Find the target layer in the model\"\"\"\n",
        "    target_layer = None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and module.out_features == 4:\n",
        "            target_layer = module\n",
        "            break\n",
        "        elif isinstance(module, nn.Conv2d):\n",
        "            target_layer = module\n",
        "    if target_layer is None:\n",
        "        raise ValueError(\"Could not find the target layer\")\n",
        "    return target_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsUr6j8ZvKKq"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary that maps label indices to class names\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "test_iterator = iter(test_loader)\n",
        "test_data = [next(test_iterator) for i in range(7)]\n",
        "\n",
        "for i, (images, labels) in enumerate(test_data):\n",
        "    images = images.to(device)\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(10, 5))\n",
        "    fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "    for j, image in enumerate(images):\n",
        "        # Generate the GradCAM heatmap\n",
        "        target_layer = find_target_layer(efficientnet)\n",
        "        heatmap = gradcam(image.unsqueeze(0), efficientnet, target_layer)\n",
        "        heatmap = cv2.resize(heatmap, (224, 224))\n",
        "        heatmap = heatmap - np.min(heatmap)\n",
        "        heatmap = heatmap / np.max(heatmap)\n",
        "\n",
        "        # Apply the heatmap to the original image\n",
        "        image = image.cpu().numpy().transpose(1, 2, 0)\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "        heatmap = np.float32(heatmap) / 255\n",
        "        image_with_heatmap = heatmap + np.float32(image)\n",
        "        image_with_heatmap = image_with_heatmap / np.max(image_with_heatmap)\n",
        "\n",
        "        # Get the predicted and true classes\n",
        "                # Get the predicted and true classes\n",
        "        with torch.no_grad():\n",
        "            outputs = efficientnet(torch.unsqueeze(image, 0))\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "        pred_class = class_names[preds[0].item()]\n",
        "        true_class = class_names[labels[j]]\n",
        "\n",
        "        # Plot the image with heatmap and label\n",
        "        axs[j // 4, j % 4].imshow(image_with_heatmap)\n",
        "        axs[j // 4, j % 4].set_title(f\"Predicted: {pred_class}\\nTrue: {true_class}\")\n",
        "        axs[j // 4, j % 4].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "LdRBoB8ZvKKt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from gradcam import GradCAM, GradCAMpp\n",
        "\n",
        "\n",
        "# Define a dictionary that maps label indices to class names\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "# Create an instance of the EfficientNet-B0 model\n",
        "# efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "# efficientnet.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "# efficientnet.eval()\n",
        "\n",
        "target_layer = efficientnet._fc\n",
        "\n",
        "# Create an instance of the GradCAM class\n",
        "gradcam = GradCAM( arch=efficientnet,target_layer=target_layer)\n",
        "\n",
        "test_iterator = iter(test_loader)\n",
        "test_data = [next(test_iterator) for i in range(7)]\n",
        "\n",
        "print(dir(gradcam))\n",
        "# print(type(gradcam))\n",
        "print(images[j:j+1].shape)\n",
        "\n",
        "\n",
        "\n",
        "for i, (images, labels) in enumerate(test_data):\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = efficientnet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    images = images.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(10, 5))\n",
        "    fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "    for j, image in enumerate(images):\n",
        "        pred_class = class_names[preds[j]]\n",
        "        true_class = class_names[labels[j]]\n",
        "        axs[j // 4, j % 4].imshow(np.transpose(image, (1, 2, 0)))\n",
        "        axs[j // 4, j % 4].set_title(f\"Predicted: {pred_class}\\nTrue: {true_class}\")\n",
        "        axs[j // 4, j % 4].axis(\"off\")\n",
        "\n",
        "        # Generate heatmap using forward method\n",
        "        output = gradcam.forward(images[j:j+1])\n",
        "#       output = gradcam(images[j:j+1])\n",
        "        pred_class_idx = preds[j]\n",
        "        pred_class_output = output[:, pred_class_idx]\n",
        "        gradcam.backward(pred_class_output)\n",
        "        heatmap = gradcam.generate()\n",
        "\n",
        "        # Visualize heatmap on the original image\n",
        "        superimposed_image = np.transpose(image, (1, 2, 0))\n",
        "        superimposed_image = np.uint8(255 * superimposed_image)\n",
        "        heatmap = cv2.resize(heatmap, (superimposed_image.shape[1], superimposed_image.shape[0]))\n",
        "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "        superimposed_image = cv2.addWeighted(superimposed_image, 0.5, heatmap, 0.5, 0)\n",
        "        axs[j // 4, j % 4 + 4].imshow(superimposed_image)\n",
        "        axs[j // 4, j % 4 + 4].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Remove hooks to prevent memory leaks\n",
        "gradcam.remove_hooks()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJrEjN56vKKt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from gradcam import GradCAMpp\n",
        "\n",
        "# Define a dictionary that maps label indices to class names\n",
        "class_names = {0: \"heavy plastic\", 1: \"no image\", 2: \"no_plastic\", 3: \"some_plastic\"}\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "efficientnet.eval()\n",
        "\n",
        "# Create an instance of the GradCAMpp class\n",
        "gradcam_pp = GradCAMpp(model=efficientnet, target_layer='blocks[-1].project.conv.weight')\n",
        "\n",
        "test_iterator = iter(test_loader)\n",
        "test_data = [next(test_iterator) for i in range(7)]\n",
        "\n",
        "for i, (images, labels) in enumerate(test_data):\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = efficientnet(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    images = images.cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(10, 5))\n",
        "    fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
        "    for j, image in enumerate(images):\n",
        "        pred_class = class_names[preds[j]]\n",
        "        true_class = class_names[labels[j]]\n",
        "        axs[j // 4, j % 4].imshow(np.transpose(image, (1, 2, 0)))\n",
        "        axs[j // 4, j % 4].set_title(f\"Predicted: {pred_class}\\nTrue: {true_class}\")\n",
        "        axs[j // 4, j % 4].axis(\"off\")\n",
        "\n",
        "        # Generate GradCAMpp heatmap\n",
        "        heatmap = gradcam_pp.generate_heatmap(images[j:j+1], target_class=preds[j])\n",
        "\n",
        "        # Visualize heatmap on the original image\n",
        "        superimposed_image = np.transpose(image, (1, 2, 0))\n",
        "        superimposed_image = np.uint8(255 * superimposed_image)\n",
        "        heatmap = cv2.resize(heatmap, (superimposed_image.shape[1], superimposed_image.shape[0]))\n",
        "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "        superimposed_image = cv2.addWeighted(superimposed_image, 0.5, heatmap, 0.5, 0)\n",
        "        axs[j // 4, j % 4 + 4].imshow(superimposed_image)\n",
        "        axs[j // 4, j % 4 + 4].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Remove hooks to prevent memory leaks\n",
        "gradcam_pp.remove_hooks()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpRvzhgCvKKu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Trial",
      "language": "python",
      "name": "plastic"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}